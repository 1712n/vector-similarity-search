// ⚠️ This file is auto-generated by wall-e (https://github.com/1712n/wall-e/).
// Do not edit it directly — instead, update the associated test/index.spec.* file and regenerate the code.

import { Client } from "pg";
type Env = {
  AI: {
    run: (
      model: string,
      input: { text: string[] },
    ) => Promise<{ data: number[][] }>;
  };
  HYPERDRIVE: { connectionString: string };
};
const MODEL = "@cf/baai/bge-base-en-v1.5";
const BATCH = 100;
const log = (lvl: string, msg: string, ctx?: Record<string, unknown>) =>
  console.log(JSON.stringify({ lvl, msg, ...ctx }));
async function getBatch(client: Client) {
  return client.query<{ id: number; content: string }>(
    `SELECT DISTINCT um.id,um.content
FROM unique_messages um
JOIN message_feed mf ON mf.message_id=um.id
WHERE um.embedding IS NULL
AND mf.timestamp>NOW()-INTERVAL '1 day'
AND um.content<>''
LIMIT $1`,
    [BATCH],
  );
}
async function updateEmbeddings(
  client: Client,
  ids: number[],
  vectors: string[],
) {
  await client.query(
    `UPDATE unique_messages SET embedding=e.embedding
FROM UNNEST($1::int[],$2::text[]) e(id,embedding)
WHERE unique_messages.id=e.id`,
    [ids, vectors],
  );
}
async function insertScores(client: Client, ids: number[]) {
  await client.query(
    `WITH pairs AS (SELECT DISTINCT topic,industry FROM synth_data_prod),
msgs AS (SELECT id,embedding FROM unique_messages WHERE id=ANY($1::int[]))
INSERT INTO message_scores(topic,industry,main,similarity,message_id)
SELECT p.topic,p.industry,NULL,
(SELECT MIN(sd.embedding<=>m.embedding) FROM synth_data_prod sd WHERE sd.topic=p.topic AND sd.industry=p.industry),
m.id
FROM msgs m CROSS JOIN pairs p`,
    [ids],
  );
  await client.query(
    `UPDATE message_scores SET main=similarity WHERE main IS NULL AND message_id=ANY($1::int[])`,
    [ids],
  );
}
export default {
  scheduled: async (_: ScheduledController, env: Env) => {
    const client = new Client({
      connectionString: env.HYPERDRIVE.connectionString,
    });
    try {
      await client.connect();
      for (;;) {
        let rows;
        try {
          rows = (await getBatch(client)).rows;
        } catch (e) {
          log("ERROR", "select_batch", { err: (e as Error).message });
          break;
        }
        if (!rows.length) {
          log("INFO", "no_new_messages");
          break;
        }
        const ids = rows.map((r) => r.id);
        const texts = rows.map((r) => r.content);
        let embeddings: number[][];
        try {
          embeddings = (await env.AI.run(MODEL, { text: texts })).data;
        } catch (e) {
          log("ERROR", "embedding_fetch", {
            err: (e as Error).message,
            count: texts.length,
          });
          break;
        }
        if (embeddings.length !== ids.length) {
          log("ERROR", "embedding_mismatch", {
            ids: ids.length,
            emb: embeddings.length,
          });
          break;
        }
        const vectors = embeddings.map((v) => `[${v.join(",")}]`);
        try {
          await updateEmbeddings(client, ids, vectors);
        } catch (e) {
          log("ERROR", "update_embeddings", {
            err: (e as Error).message,
            count: ids.length,
          });
          break;
        }
        try {
          await insertScores(client, ids);
        } catch (e) {
          log("ERROR", "insert_scores", {
            err: (e as Error).message,
            count: ids.length,
          });
          break;
        }
        log("INFO", "processed_batch", { count: ids.length });
        if (rows.length < BATCH) break;
      }
    } catch (e) {
      log("ERROR", "worker_failure", { err: (e as Error).message });
    } finally {
      try {
        await client.end();
      } catch {}
    }
  },
};
